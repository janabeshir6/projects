{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6393bd4e",
   "metadata": {},
   "source": [
    "## setup and load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845bb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['mol_id', 'smiles', 'A', 'B', 'C', 'mu', 'alpha', 'homo', 'lumo', 'gap', 'r2', 'zpve', 'u0', 'u298', 'h298', 'g298', 'cv', 'u0_atom', 'u298_atom', 'h298_atom', 'g298_atom']\n",
      "  smiles     gap      mu\n",
      "0      C  0.5048  0.0000\n",
      "1      N  0.3399  1.6256\n",
      "2      O  0.3615  1.8511\n",
      "3    C#C  0.3351  0.0000\n",
      "4    C#N  0.3796  2.8937\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load QM9 dataset\n",
    "url = 'https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/qm9.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Inspect basic info\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "#smiles (Molecule string), gap (HOMO-LUMO gap), mu (dipole moment)\n",
    "df = df[['smiles', 'gap', 'mu']]  # Keep only what's needed\n",
    "df = df.dropna()\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2969ee",
   "metadata": {},
   "source": [
    "## convert SMILES to SELFIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f9b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safely encode SMILES to SELFIES, allowing invalids to return None\n",
    "import selfies as sf #Self-Referencing Embedded Strings (Used to tokenize)\n",
    "import rdkit as rd #open-source toolkit for cheminformatics\n",
    "def safe_encode(smiles):\n",
    "    try:\n",
    "        return sf.encoder(smiles, strict=False)\n",
    "    except sf.EncoderError:\n",
    "        return None\n",
    "\n",
    "df['selfies'] = df['smiles'].apply(safe_encode)\n",
    "\n",
    "# Drop rows where encoding failed\n",
    "df = df[df['selfies'].notnull()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7d699",
   "metadata": {},
   "source": [
    "## tokenize SELFIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9611b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize to split the molecules into atoms\n",
    "# Build vocabulary to map the tokens to\n",
    "all_tokens = set() #stores unique tokens from all SELFIES\n",
    "tokenized = [] #List of token sequences\n",
    "max_len = 0\n",
    "\n",
    "for s in df['selfies']:\n",
    "    tokens = list(sf.split_selfies(s)) #Splits each SELFIES string into a list of tokens.\n",
    "    all_tokens.update(tokens) #Updates vocabulary\n",
    "    tokenized.append(tokens)\n",
    "    max_len = max(max_len, len(tokens)) #Keeps track of the maximum sequence length.\n",
    "\n",
    "\n",
    "# Index maps\n",
    "token2idx = {tok: i + 2 for i, tok in enumerate(sorted(all_tokens))}\n",
    "token2idx[\"<PAD>\"] = 0 #Padding\n",
    "token2idx[\"<SOS>\"] = 1 #Start of sequence\n",
    "idx2token = {i: t for t, i in token2idx.items()} #maps index back to its token\n",
    "vocab_size = len(token2idx) #Total number of unique tokens\n",
    "\n",
    "# Encode sequences\n",
    "def encode(tokens):\n",
    "    ids = [token2idx[\"<SOS>\"]] + [token2idx[t] for t in tokens]\n",
    "    return ids + [0] * (max_len - len(tokens))\n",
    "\n",
    "encoded = [encode(t) for t in tokenized] #Encodes all tokenized sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1933dd01",
   "metadata": {},
   "source": [
    "## PREPARE DATA FOR TRAINING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c1f1570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.sequences[idx]\n",
    "\n",
    "dataset = MoleculeDataset(encoded)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8125b10",
   "metadata": {},
   "source": [
    "## build VAE model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    #Token embedding size --> used as input for the encoder\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden_dim=256, latent_dim=64):\n",
    "        super(VAE, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim) #Turns token indices into vectors\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True) #Compresses input to hidden vector\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim) #Mean of latent distribution\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim) #log-variance (Uncertainty of distribution)\n",
    "\n",
    "        # Decoder\n",
    "        self.latent2hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True) #Reads in <SOS> tokens\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size) #GRU output --> Predicted tokens\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h = self.encoder_rnn(x)\n",
    "        h = h.squeeze(0)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    #Chooses a random point z using mu and logvar, where z is a compressed version of the input (Used to generate new SELFIES)\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    #Decodes z to a sequence of logits\n",
    "    def decode(self, z, seq_len):\n",
    "        hidden = self.latent2hidden(z).unsqueeze(0)\n",
    "        inputs = torch.full((z.size(0), seq_len), token2idx['<SOS>'], dtype=torch.long).to(z.device)\n",
    "        emb = self.embedding(inputs)\n",
    "        out, _ = self.decoder_rnn(emb, hidden)\n",
    "        return self.output(out)\n",
    "\n",
    "    def forward(self, x): #VAE forward pass\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        logits = self.decode(z, x.size(1)) #raw, unprocessed output of a model before it's turned into a probability\n",
    "        return logits, mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ec13b7",
   "metadata": {},
   "source": [
    "## train the VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ae789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 133885\n",
      "Number of batches: 2092\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2092/2092 [04:21<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Total Loss = 1.6980 | Recon = 1.6954 | KL = 0.0025\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2092/2092 [04:56<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Total Loss = 1.6747 | Recon = 1.6745 | KL = 0.0002\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2092/2092 [09:46<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Total Loss = 1.6634 | Recon = 1.6633 | KL = 0.0001\n",
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2092/2092 [08:09<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Total Loss = 1.6641 | Recon = 1.6626 | KL = 0.0015\n",
      "\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2092/2092 [10:04<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Total Loss = 1.6619 | Recon = 1.6618 | KL = 0.0000\n",
      "\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2092/2092 [08:06<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Total Loss = 1.6616 | Recon = 1.6615 | KL = 0.0000\n",
      "\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2092/2092 [08:48<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Total Loss = 1.6617 | Recon = 1.6617 | KL = 0.0000\n",
      "\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2092/2092 [09:15<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Total Loss = 1.6613 | Recon = 1.6613 | KL = 0.0000\n",
      "\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2092/2092 [10:09<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Total Loss = 1.6614 | Recon = 1.6610 | KL = 0.0004\n",
      "\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2092/2092 [04:17<00:00,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Total Loss = 1.6608 | Recon = 1.6608 | KL = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE(vocab_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_fn(recon_logits, target, mu, logvar):\n",
    "    #Compares output vs target\n",
    "    recon_loss = F.cross_entropy(recon_logits.view(-1, vocab_size), target.view(-1), ignore_index=0)\n",
    "    #Ensures latent space is Gaussian (Normal distribution)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / target.size(0)\n",
    "    return recon_loss + kl_loss, recon_loss, kl_loss\n",
    "\n",
    "# Print dataset and batch info\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(loader)}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, 11):  # 1 to 10 inclusive\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_recon = 0\n",
    "    total_kl = 0\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}\")\n",
    "\n",
    "    #Forward → loss → backprop → optimizer step and Tracks total loss.\n",
    "    for batch_idx, (x, y) in enumerate(tqdm(loader)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, mu, logvar = model(x)\n",
    "        loss, recon_loss, kl_loss = loss_fn(logits, y, mu, logvar)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_recon += recon_loss.item()\n",
    "        total_kl += kl_loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}: Total Loss = {total_loss/len(loader):.4f} | Recon = {total_recon/len(loader):.4f} | KL = {total_kl/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce79ec1",
   "metadata": {},
   "source": [
    "## GENERATE NEW MOLECULES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922847fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import selfies as sf\n",
    "\n",
    "# This should have been calculated during tokenization\n",
    "max_len = max(len(t) for t in tokenized)  # tokenized = list of token lists\n",
    "MAX_LEN = max_len  # define global name if needed\n",
    "\n",
    "#Samples 100 random points in latent space and decodes each into a sequence of tokens.\n",
    "#Converts token sequences → SELFIES → SMILES.\n",
    "def generate_molecules(model, num_samples=100, latent_dim=64, max_len=MAX_LEN, device='cpu'):\n",
    "    model.eval()\n",
    "    z = torch.randn(num_samples, latent_dim).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model.decode(z, max_len)  # (batch, seq_len, vocab_size)\n",
    "        probs = F.softmax(logits, dim=-1)  # Turn logits into probabilities\n",
    "        \n",
    "        # Multinomial sampling from the probability distribution\n",
    "        sampled_ids = torch.zeros_like(logits.argmax(dim=-1), dtype=torch.long)\n",
    "        for i in range(logits.size(1)):  # for each position in sequence\n",
    "            sampled_ids[:, i] = torch.multinomial(probs[:, i], num_samples=1).squeeze()\n",
    "\n",
    "    # Convert token IDs → SELFIES → SMILES\n",
    "    generated_selfies = []\n",
    "    for seq in sampled_ids.cpu().numpy():\n",
    "        tokens = [idx2token.get(i, '') for i in seq if i > 1]\n",
    "        selfies_str = \"\".join(tokens)\n",
    "        generated_selfies.append(selfies_str)\n",
    "\n",
    "    generated_smiles = [sf.decoder(s) for s in generated_selfies]\n",
    "    return generated_smiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12c9c8",
   "metadata": {},
   "source": [
    "## VALIDATE & EVALUATE MOLECULES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Molecule Evaluation ===\n",
      "1: ✅ Valid SMILES - C1=C2CC=C3C1CC3ON2\n",
      "    QED: 0.502, logP: 1.12\n",
      "2: ✅ Valid SMILES - C1=COON1\n",
      "    QED: 0.407, logP: -0.08\n",
      "3: ✅ Valid SMILES - C1=NCCN=CCC1\n",
      "    QED: 0.443, logP: 0.92\n",
      "4: ✅ Valid SMILES - C=CCCC=COC#N\n",
      "    QED: 0.248, logP: 1.96\n",
      "5: ✅ Valid SMILES - CC1NCCCOOC1C\n",
      "    QED: 0.509, logP: 0.70\n",
      "6: ✅ Valid SMILES - CCN=CCN(N)OO\n",
      "    QED: 0.238, logP: -0.34\n",
      "7: ✅ Valid SMILES - C1=C2COCCC12\n",
      "    QED: 0.408, logP: 0.96\n",
      "8: ✅ Valid SMILES - C1=C2C3CC(C1)C2C3\n",
      "    QED: 0.414, logP: 1.97\n",
      "9: ✅ Valid SMILES - CCC=C=N\n",
      "    QED: 0.447, logP: 1.20\n",
      "10: ✅ Valid SMILES - CC=CCCCC=COCF\n",
      "    QED: 0.327, logP: 3.19\n",
      "11: ✅ Valid SMILES - C1=C2CC12\n",
      "    QED: 0.360, logP: 0.95\n",
      "12: ✅ Valid SMILES - C1=CC2OCC2CCCCC1\n",
      "    QED: 0.485, logP: 2.52\n",
      "13: ✅ Valid SMILES - N1=NC2OC12\n",
      "    QED: 0.373, logP: 0.13\n",
      "14: ✅ Valid SMILES - CCC=C=NCNC1=NC1\n",
      "    QED: 0.567, logP: 0.58\n",
      "15: ✅ Valid SMILES - C1=C2C3=C1N2OCC3\n",
      "    QED: 0.453, logP: 0.79\n",
      "16: ✅ Valid SMILES - CCCCOCOCCC#N\n",
      "    QED: 0.417, logP: 1.69\n",
      "17: ✅ Valid SMILES - CC1=C=N1\n",
      "    QED: 0.383, logP: 0.57\n",
      "18: ✅ Valid SMILES - CCC=CCCNC#N\n",
      "    QED: 0.266, logP: 1.41\n",
      "19: ✅ Valid SMILES - CC=CC\n",
      "    QED: 0.370, logP: 1.58\n",
      "20: ✅ Valid SMILES - C1=C2CCOC12\n",
      "    QED: 0.391, logP: 0.72\n",
      "21: ✅ Valid SMILES - C=C=C1C#CC1\n",
      "    QED: 0.299, logP: 1.10\n",
      "22: ✅ Valid SMILES - CCCOCCOC#N\n",
      "    QED: 0.409, logP: 0.91\n",
      "23: ✅ Valid SMILES - C#CC1CCCC2=C1CCOO2\n",
      "    QED: 0.403, logP: 2.03\n",
      "24: ✅ Valid SMILES - N=COOCC#CCCCO\n",
      "    QED: 0.151, logP: 0.32\n",
      "25: ✅ Valid SMILES - CCC(C)CCCCCC#N\n",
      "    QED: 0.536, logP: 3.51\n",
      "26: ✅ Valid SMILES - NC12CCOC1C2\n",
      "    QED: 0.459, logP: -0.12\n",
      "27: ✅ Valid SMILES - C=C1CCCCOC1\n",
      "    QED: 0.434, logP: 1.74\n",
      "28: ✅ Valid SMILES - CC1OC2=C1CCON2\n",
      "    QED: 0.515, logP: 0.54\n",
      "29: ✅ Valid SMILES - C1CCOCCCNOCC1\n",
      "    QED: 0.574, logP: 1.10\n",
      "30: ✅ Valid SMILES - CCOC=O\n",
      "    QED: 0.437, logP: 0.18\n",
      "31: ✅ Valid SMILES - C1=COCN=C1\n",
      "    QED: 0.419, logP: 0.56\n",
      "32: ✅ Valid SMILES - CC=CC1OCNOCO1\n",
      "    QED: 0.541, logP: 0.37\n",
      "33: ✅ Valid SMILES - C=C=CCCF\n",
      "    QED: 0.448, logP: 1.69\n",
      "34: ✅ Valid SMILES - N=CCNO\n",
      "    QED: 0.309, logP: -0.39\n",
      "35: ✅ Valid SMILES - CC=NCCCCC=O\n",
      "    QED: 0.312, logP: 1.45\n",
      "36: ✅ Valid SMILES - CCCC1CC2CC12\n",
      "    QED: 0.512, logP: 2.44\n",
      "37: ✅ Valid SMILES - CC1=CNC=CCONCN=C1\n",
      "    QED: 0.554, logP: 0.56\n",
      "38: ✅ Valid SMILES - CC1=CON=CC1\n",
      "    QED: 0.446, logP: 1.30\n",
      "39: ✅ Valid SMILES - O=O\n",
      "    QED: 0.378, logP: 0.07\n",
      "40: ✅ Valid SMILES - CC12CC1C2\n",
      "    QED: 0.402, logP: 1.42\n",
      "41: ✅ Valid SMILES - C=C=C\n",
      "    QED: 0.353, logP: 0.96\n",
      "42: ✅ Valid SMILES - C=CC1C2NNCOC12\n",
      "    QED: 0.470, logP: -0.38\n",
      "43: ✅ Valid SMILES - CCCCOCNF\n",
      "    QED: 0.335, logP: 1.23\n",
      "44: ✅ Valid SMILES - C1CC23OCC2C32CCC12\n",
      "    QED: 0.491, logP: 1.58\n",
      "45: ✅ Valid SMILES - C=C=CC#CC1CCCOO1\n",
      "    QED: 0.295, logP: 1.44\n",
      "46: ✅ Valid SMILES - CCCCC=O\n",
      "    QED: 0.374, logP: 1.38\n",
      "47: ✅ Valid SMILES - C=CC(C)C1C=CON1\n",
      "    QED: 0.560, logP: 1.23\n",
      "48: ✅ Valid SMILES - C1=COC1\n",
      "    QED: 0.393, logP: 0.53\n",
      "49: ✅ Valid SMILES - C#CCOCCC#CCCOCCCC\n",
      "    QED: 0.450, logP: 2.24\n",
      "50: ✅ Valid SMILES - CCCC=CCN1CO1\n",
      "    QED: 0.420, logP: 1.55\n",
      "51: ✅ Valid SMILES - CC1=COCON=CCC1\n",
      "    QED: 0.513, logP: 1.66\n",
      "52: ✅ Valid SMILES - CCCCCCCCC#N\n",
      "    QED: 0.518, logP: 3.26\n",
      "53: ✅ Valid SMILES - C=C(C)CC1CCCCC=C1C\n",
      "    QED: 0.539, logP: 4.09\n",
      "54: ✅ Valid SMILES - CC=CCC1CO1\n",
      "    QED: 0.375, logP: 1.35\n",
      "55: ✅ Valid SMILES - ON1CC#COC1\n",
      "    QED: 0.420, logP: -0.37\n",
      "56: ✅ Valid SMILES - C1C2CC12\n",
      "    QED: 0.387, logP: 1.03\n",
      "57: ✅ Valid SMILES - C1=C2CCC12\n",
      "    QED: 0.374, logP: 1.34\n",
      "58: ✅ Valid SMILES - OC#CCO\n",
      "    QED: 0.372, logP: -0.69\n",
      "59: ✅ Valid SMILES - CC1OC2CCC=C12\n",
      "    QED: 0.429, logP: 1.49\n",
      "60: ✅ Valid SMILES - O=C1CCOOC1\n",
      "    QED: 0.401, logP: -0.09\n",
      "61: ✅ Valid SMILES - C(#CC1C#C1)CC1CCCC1\n",
      "    QED: 0.495, logP: 2.20\n",
      "62: ✅ Valid SMILES - CC(CO)CCC#CCO\n",
      "    QED: 0.560, logP: 0.39\n",
      "63: ✅ Valid SMILES - CCCCCCOCC=O\n",
      "    QED: 0.402, logP: 1.78\n",
      "64: ✅ Valid SMILES - CCNCCCC1=CO1\n",
      "    QED: 0.562, logP: 1.25\n",
      "65: ✅ Valid SMILES - COC1CC=CC1\n",
      "    QED: 0.450, logP: 1.35\n",
      "66: ✅ Valid SMILES - CCCCCOCCC(C)=COCO\n",
      "    QED: 0.354, logP: 2.45\n",
      "67: ✅ Valid SMILES - C=CCCNOC1C2CCC21\n",
      "    QED: 0.367, logP: 1.49\n",
      "68: ✅ Valid SMILES - C=C1CC1\n",
      "    QED: 0.366, logP: 1.34\n",
      "69: ✅ Valid SMILES - CC1NCCO1\n",
      "    QED: 0.447, logP: -0.05\n",
      "70: ✅ Valid SMILES - CCOC=C=C=C=O\n",
      "    QED: 0.297, logP: 0.68\n",
      "71: ✅ Valid SMILES - C1=C2C3=C2C(CCC1)CC3\n",
      "    QED: 0.475, logP: 2.82\n",
      "72: ✅ Valid SMILES - C1=CNOC1\n",
      "    QED: 0.435, logP: 0.03\n",
      "73: ✅ Valid SMILES - C1=NC1\n",
      "    QED: 0.363, logP: 0.07\n",
      "74: ✅ Valid SMILES - CCCCC1C=CC1\n",
      "    QED: 0.490, logP: 2.75\n",
      "75: ✅ Valid SMILES - CC=CCC=O\n",
      "    QED: 0.362, logP: 1.15\n",
      "76: ✅ Valid SMILES - COCOC=O\n",
      "    QED: 0.272, logP: -0.24\n",
      "77: ✅ Valid SMILES - CC#CCC1C2OC(C)C12\n",
      "    QED: 0.495, logP: 1.43\n",
      "78: ✅ Valid SMILES - COCNCCCCCC=O\n",
      "    QED: 0.325, logP: 0.94\n",
      "79: ✅ Valid SMILES - FC1COOC1\n",
      "    QED: 0.400, logP: 0.29\n",
      "80: ✅ Valid SMILES - CC1CCCC2CC12\n",
      "    QED: 0.449, logP: 2.44\n",
      "81: ✅ Valid SMILES - C=C=CNCNCN=C\n",
      "    QED: 0.236, logP: 0.08\n",
      "82: ✅ Valid SMILES - CCC1=CCC2C(C)C3CC123\n",
      "    QED: 0.501, logP: 3.00\n",
      "83: ✅ Valid SMILES - C#CCCN=O\n",
      "    QED: 0.275, logP: 0.78\n",
      "84: ✅ Valid SMILES - C1=NC2C=C2C1\n",
      "    QED: 0.380, logP: 0.77\n",
      "85: ✅ Valid SMILES - CCC1CCCCCO1\n",
      "    QED: 0.526, logP: 2.36\n",
      "86: ✅ Valid SMILES - C1CC2NCC12\n",
      "    QED: 0.446, logP: 0.37\n",
      "87: ✅ Valid SMILES - C1OC2C34CCC23C14\n",
      "    QED: 0.446, logP: 0.80\n",
      "88: ✅ Valid SMILES - C=CN=NCOC#N\n",
      "    QED: 0.310, logP: 1.04\n",
      "89: ✅ Valid SMILES - C1=NC2CCC12\n",
      "    QED: 0.410, logP: 0.85\n",
      "90: ✅ Valid SMILES - C1=C2CCCC2CC1\n",
      "    QED: 0.418, logP: 2.51\n",
      "91: ✅ Valid SMILES - C1=COCC1\n",
      "    QED: 0.412, logP: 0.92\n",
      "92: ✅ Valid SMILES - C1CC2CN3COC(O2)C3C1\n",
      "    QED: 0.510, logP: 0.55\n",
      "93: ✅ Valid SMILES - NC=CC1C=CCO1\n",
      "    QED: 0.497, logP: 0.41\n",
      "94: ✅ Valid SMILES - C#CCCC=O\n",
      "    QED: 0.271, logP: 0.60\n",
      "95: ✅ Valid SMILES - CC(O)CN1C=CCO1\n",
      "    QED: 0.575, logP: 0.13\n",
      "96: ✅ Valid SMILES - C1CC2C3OOC1N23\n",
      "    QED: 0.328, logP: 0.08\n",
      "97: ✅ Valid SMILES - C1=CC2CC=CCC2CC1\n",
      "    QED: 0.447, logP: 2.92\n",
      "98: ✅ Valid SMILES - NCOOO\n",
      "    QED: 0.261, logP: -0.68\n",
      "99: ✅ Valid SMILES - CC1=C=N1\n",
      "    QED: 0.383, logP: 0.57\n",
      "100: ✅ Valid SMILES - CN1CCOCCCCCC=N1\n",
      "    QED: 0.550, logP: 1.49\n",
      "\n",
      "=== Summary ===\n",
      "Total Generated: 100\n",
      "✅ Valid: 100 (100.00%)\n",
      "🔁 Unique: 99 (99.00%)\n",
      "🆕 Novel: 91 (91.00%)\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Generate 100 molecules using the improved sampling method\n",
    "generated_smiles = generate_molecules(\n",
    "    model,\n",
    "    num_samples=100,\n",
    "    latent_dim=64,\n",
    "    max_len=max_len,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# STEP 2: Run your evaluation block (validity, uniqueness, novelty, qed, logP)\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import QED, Crippen\n",
    "\n",
    "valid_smiles = []\n",
    "seen_set = set()\n",
    "novel_set = set()\n",
    "qed_list = [] #Drug likeness\n",
    "logp_list = [] #Hydrophobicity\n",
    "\n",
    "train_smiles_set = set(df['smiles']) #Used to determine if it's new\n",
    "\n",
    "print(\"=== Generated Molecule Evaluation ===\")\n",
    "\n",
    "#Checks which SMILES are chemically valid\n",
    "for i, smi in enumerate(generated_smiles):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol:\n",
    "        canonical = Chem.MolToSmiles(mol)\n",
    "        valid_smiles.append(canonical)\n",
    "\n",
    "        qed = QED.qed(mol)\n",
    "        logp = Crippen.MolLogP(mol)\n",
    "        qed_list.append(qed)\n",
    "        logp_list.append(logp)\n",
    "\n",
    "        print(f\"{i+1}: ✅ Valid SMILES - {canonical}\")\n",
    "        print(f\"    QED: {qed:.3f}, logP: {logp:.2f}\")\n",
    "\n",
    "        if canonical not in train_smiles_set:\n",
    "            novel_set.add(canonical)\n",
    "\n",
    "        seen_set.add(canonical)\n",
    "    else:\n",
    "        print(f\"{i+1}: ❌ Invalid SMILES\")\n",
    "\n",
    "# STEP 3: Evaluation metrics\n",
    "total = len(generated_smiles)\n",
    "valid = len(valid_smiles)\n",
    "unique = len(seen_set)\n",
    "novel = len(novel_set)\n",
    "\n",
    "validity = 100 * valid / total\n",
    "uniqueness = 100 * unique / valid if valid else 0\n",
    "novelty = 100 * novel / valid if valid else 0\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Total Generated: {total}\")\n",
    "print(f\"✅ Valid: {valid} ({validity:.2f}%)\")\n",
    "print(f\"🔁 Unique: {unique} ({uniqueness:.2f}%)\")\n",
    "print(f\"🆕 Novel: {novel} ({novelty:.2f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
